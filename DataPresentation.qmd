---
title: "US Real Estate Market Presentation"
date: today
author: 
  - Emir Hamulic
  - Elias Gabriel
self-contained: true
code-link: true
code-tools: true
lightbox: true  ## click on image to open in lightbox
format: 
  revealjs:
    scrollable: true
    smaller: true
    theme: sky ##https://quarto.org/docs/presentations/revealjs/themes.html
---

```{r}
#| code-summary: Libraries
library <- function(...) {suppressPackageStartupMessages(base::library(...))}
library(tidyverse)
library(dplyr)
library(knitr)
library(tidyr)
library(rmarkdown)
library(janitor)
library(scales)
library(tidytext)
library(ggforce)
library(GGally)
library(DT)
library(kableExtra)
library(broom)
library(plotly)
```

```{r}
data = read.csv("data/realtor-data.zip.csv") # Data import
data = subset(data, select = c(status, price, bed, bath, acre_lot, city, state, house_size)) # keep relevant columns
data$status = as.factor(data$status)
data$city = as.factor(data$city)
data$state = as.factor(data$state)
data <- na.omit(data)
data = data |>
  filter(price > 10000 & price < 1000000000)
data = data |>
  mutate(price_per_sqm = price/house_size)
```

## Basics

-   Dataset of 2.2M+ real estate listings across the United States (source: Kaggle, Ahmed Shahriar Sakib).
-   Includes key features: price, size, lot area, bedrooms, bathrooms, location, and status.
-   Goals:
    -   Understand price patterns and housing characteristics across states.
    -   Analyze relationships between property features and regional affordability trends.
    -   Provide insights into the current U.S. housing market dynamics.
    -   Train and compare different Regression Models for price prediction

## Data structure

```{r}
#| label: "data_structure_after_transformation"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) as.character(val) else as.character(round(val, 2))
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

## Distribution of Numerical Variables

```{r}
#| label: "Logarithmic Visualisation"
#| tbl-cap: "Visualisation of numerical variables in dataframe"
#| code-fold: true
data |>
  clean_names() |>
  pivot_longer(cols = where(is.numeric)) |>
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "white") +
  scale_x_log10(labels = label_comma()) +   # ðŸ‘ˆ echte Werte, log-Skala
  facet_wrap(~ name, scales = "free_x") +
  theme_minimal() +
  labs(
    title = "Distribution of Numerical Variables (logarithmic scale)",
    x = "Value",
    y = "Count"
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 25, hjust = 1) 
    )
```

## Top Categories per Variable

```{r}
#| label: "Visualization Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

top_n_per_var <- 10 

nominal_summary <- data |>
  clean_names() |>
  select(where(is.factor), price) |>
  pivot_longer(cols = where(is.factor),
               names_to = "Variable",
               values_to = "Category") |>
  group_by(Variable, Category) |>
  summarise(
    Count = n(),
    Percent = round(100 * Count / nrow(data), 2),
    Mean_Price = round(mean(price, na.rm = TRUE), 0),
    .groups = "drop"
  ) |>
  group_by(Variable) |>
  slice_max(order_by = Count, n = top_n_per_var, with_ties = FALSE) |>
  ungroup()


nominal_summary <- nominal_summary |>
  group_by(Variable) |>
  mutate(Category = forcats::fct_reorder(Category, Count),
         Category = factor(Category, levels = unique(Category))) |>
  ungroup()

# Plot: Facets untereinander, mit eigener x-Skala und y-Skala pro Variable
ggplot(nominal_summary, aes(x = Count, y = Category, fill = Variable)) +
  geom_col(show.legend = FALSE, alpha = 0.8, width = 0.7) +
  facet_wrap(~ Variable, ncol = 1, scales = "free", drop = TRUE) +
  scale_x_continuous(labels = label_comma()) +   #Tausendertrennung, keine 1e+05
  theme_minimal() +
  labs(
    title = "Top Categories per Factor Variable",
    x = "Count",
    y = "Category"
  ) +
  theme(
    panel.spacing.y = unit(1, "lines"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8),
    plot.margin = margin(5, 15, 5, 5)
  )
```

## Data Split

```{r setup-ml, include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(ranger)

set.seed(123)

# Train/test split
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data  <- data[-train_indices, ]

# Linear Regression
model_lr <- lm(price ~ house_size + bath + bed + state + status,
               data = train_data)

# Random Forest (best params)
rf_model <- ranger(
  price ~ house_size + bed + bath + acre_lot + state + status,
  data = train_data,
  num.trees = 300,
  mtry = 2,
  importance = "impurity"
)

# Metrics
lm_preds <- predict(model_lr, test_data)
rf_preds <- predict(rf_model, data=test_data)$predictions

rmse_lm <- sqrt(mean((test_data$price - lm_preds)^2))
rmse_rf <- sqrt(mean((test_data$price - rf_preds)^2))

```

-   Dataset split into training and test sets
-   70 % Training / 30 % Test
-   Same split used for all models
-   Test data only used for final evaluation

## Linear Regression

-   Baseline regression model
-   Location represented by state (cities excluded due to many levels)
-   Strongest positive effects:
    -   Bathrooms increase price the most
    -   Larger house size increases price
    -   Some states (e.g. Hawaii, California) strongly increase price
-   Bedrooms show a negative effect when controlling for other variables
-   Effects assume all other variables remain constant
-   Test **RMSE â‰ˆ 1,018,695** â†’ underfitting

## Linear Regression

```{r}
tidy(model_lr, conf.int = TRUE) |>
  filter(term != "(Intercept)") |>
  mutate(term = reorder(term, estimate)) |>
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal()
```

## Random Forest

-   Hyperparameter tuning via grid search
-   Best configuration:
    -   **300 trees**
    -   **mtry = 2**
-   More trees gave no improvement
-   Larger mtry reduced performance
-   Feature importance highlights house size, location and bathrooms
-   Test **RMSE â‰ˆ 842,861** â†’ major improvement over linear regression

## Random Forest

```{r}
importance_df <- enframe(rf_model$variable.importance, 
                         name = "Variable", value = "Importance") |>
  arrange(desc(Importance))

ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  coord_flip() +
  theme_minimal()
```

## Neural Network

-   Requires preprocessing:
    -   One-hot encoding
    -   Scaling of numeric features
    -   Handling rare and unseen categories
-   Optional log transformation of target
-   Grid search over hidden layer size and decay
-   Best parameters:
    -   **size = 7**
    -   **decay = 0.10**

## Neural Network

-   Strong regularization improved generalization
-   Test Performance:
    -   **RMSE â‰ˆ 817,135**
    -   MAE â‰ˆ 221,255
    -   RÂ² â‰ˆ 0.423
-   Most flexible model, but least interpretable

## Model Comparison

```{r}
comparison <- tibble(
  Model = c("Linear Regression", "Random Forest", "Neural Network"),
  RMSE  = c(1018695, 842861, 817135)
)

ggplot(comparison, aes(x = Model, y = RMSE)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Model Performance Comparison (Test RMSE)")
```

## Thank you for your attention