---
title: "Data Exploration Gabriel Hamulic"
subtitle: "Dataset: US Real Estate Dataset"
author: "Hamulic, Gabriel"
date: today
embed-resources: true
format:
  html:
    output-file: US-Real-Estate_Gabriel_Hamulic.html
    #output-ext: "html.html" 
    toc: true
    toc-location: right
    code-link: true
    code-tools: true
    #df-print: kable
    theme:
      light: flatly
      dark: darkly
    #echo: fenced
  pdf: 
    output-file: US-Real-Estate_Gabriel_Hamulic.pdf 
    toc: true
    number-sections: true
    code-link: true
    df-print: tibble
    crossref: 
      lof-title: "List of Figures"
fig-align: center
execute: 
  warning: false
---

\listoffigures 
\listoftables
\listoflistings

::: callout-caution
### Instructions

Your report must be of high quality, meaning that your report:

-   is visually and textually pleasing of
-   does not look/read/feel like a draft instead of a finished analysis
-   explains/discusses your findings and results in the main text, e.g., explain/discuss all figures/table in the main text
-   is representable such that it can show to any interested third party
-   uses figure/table captions/linking/reference (see example further down)
-   Do not show any standard printout of R-code, use for data.frame/tibbles `knitr::kable()` printing.
-   Do not simply print datasets (too many lines) use instead `rmarkdown::paged_table()`
:::

{{< pagebreak >}}

# Introduction

In this project, we explore a large dataset of over 2 million real estate listings across the United States. The data includes key property attributes such as price, size, lot area, location, and listing status.

Our goal is to gain a deeper understanding of the current U.S. housing market by identifying:

-   Patterns in property prices and sizes across states and cities

-   Relationships between housing characteristics (e.g., size, bedrooms, bathrooms)

-   Regional differences in affordability and market activity

## Libraries

```{r}
#| code-summary: Libraries
#| code-fold: true
library <- function(...) {suppressPackageStartupMessages(base::library(...))}
library(ranger)
library(tidyverse)
library(dplyr)
library(knitr)
library(tidyr)
library(rmarkdown)
library(janitor)
library(scales)
library(tidytext)
library(ggforce)
library(GGally)
library(DT)
library(kableExtra)
library(broom)
library(plotly)
library(nnet)


```

# Data

## Data source

In this data exploration we are looking at the US Real Estate market with use of a dataset from kaggle published by Ahmed Shahriar Sakib. It contains over 2.2 Million Real Estate listings broken down to State, Size, Price (among other factors). (Source: <https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset/data>)

## Data import

```{r setup, include=FALSE}
options(dplyr.print_max = 15, dplyr.print_min = 10)
```

```{r}
data = read.csv("data/realtor-data.zip.csv") # Data import
```

## Data Transformation

```{r}
data = subset(data, select = c(status, price, bed, bath, acre_lot, city, state, house_size)) # keep relevant columns
```

### Structure with standard datatypes

```{r}
#| label: "data_structure"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) 
      as.character(val)
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

```{r}
#| code-fold: true

# Assign Data Types
data$status = as.factor(data$status)
data$city = as.factor(data$city)
data$state = as.factor(data$state)
```

### NA Removal

```{r}
#| code-fold: true
before_rows <- nrow(data)
data <- na.omit(data)
after_rows <- nrow(data)
kable(data.frame(Description = c("Before NA removal", "After NA removal"),
Rows = c(before_rows, after_rows)))
```

The dataset now has `r nrow(data)` observations and `r ncol(data)` variables after removing rows with missing values.

### Filtering

```{r}
#| code-fold: true

# Filter min and max values
data = data |>
  filter(price > 10000 & price < 1000000000)
```

### Calculations

```{r}
#| code-fold: true

data = data |>
  mutate(price_per_sqm = price/house_size)
```

### Cleaned Dataset

```{r}
#| code-fold: true

paged_table(data)
```

### Structure after transformation

```{r}
#| label: "data_structure_after_transformation"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) as.character(val) else as.character(round(val, 2))
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

After cleaning the dataset, all variables have appropriate data types and no missing values (**n = `r nrow(data)`**):

-   **status** â€“ Factor variable showing the listing status (e.g., for_sale, sold).

-   **price** â€“ Numeric value for the propertyâ€™s price in USD.

-   **bed, bath** â€“ Integer counts of bedrooms and bathrooms.

-   **acre_lot** â€“ Numeric size of the lot (in acres).

-   **city, state** â€“ Factor variables identifying the propertyâ€™s location.

-   **house_size** â€“ Numeric size of the house (in square feet).

-   **price_per_sqm** â€“ Numeric variable derived from price / house_size to compare prices across properties.

All rows with missing data were removed, and categorical variables were converted to factors for easier analysis and visualization later on.

## Data dictionary

```{r}
#| code-fold: true

tibble(
  Variable = c("price", "status", "acre_lot", "state", "house_size", "price_per_sqm"),
  Description = c(
    "The price for which the item was listed on the market",
    "The status if the house is already sold or still for sale",
    "The size of the land / lot on which the house is located in acres",
    "The state in which the house is located",
    "The size of the house in square feet",
    "The price per square footage"
  )
) |>
  kable(
    caption = "Description of key variables in the dataset",
    align = c("l", "l")
  )
```

# Summary statistic tables

In this section we will cover the summary of our cleaned dataset. We will explore basic statistical values from our data.

## Numeric Statistics

### Summary of numerical values

```{r}
#| label: "Numeric Statistics"
#| tbl-cap: "Summary statistics of numerical variables in dataframe"
#| code-fold: true
data |> 
  janitor::clean_names() |>
  mutate(row = row_number() |> factor()) |> 
  pivot_longer(cols = where(is.numeric)) |> 
  group_by(name) |> 
  summarize(N = n(),
            min = min(value),
            mean = mean(value),
            median = median(value),
            max = max(value),
            st.dev = sd(value)
            ) |> 
  knitr::kable(digits = 2)
```

#### Interpretation

**price**: Very wide range (\$10.4kâ€“\$51.5M). Mean (\$573k), median (\$379k), indicating strong right-skew and high-priced outliers.

**house_size**: Average \~2,119 sqft, median 1,812 sqft. Extremely large max (1,560,780 sqft) signals outliers. The distribution is right-skewed.

**acre_lot**: Median 0.21 acres vs. mean 12.75 acres â†’ a few very large parcels inflate the mean.

**bed / bath**: Typical homes (\~3 beds, 2 baths) with modest spread; minima at 1 suggest realistic counts.

**price_per_sqm**: Mean \$262.42 vs. median \$197.42, also right-skewed, consistent with price outliers.

### Visualisation of numerical values

```{r}
#| label: "Logarithmic Visualisation"
#| tbl-cap: "Visualisation of numerical variables in dataframe"
#| code-fold: true
data |>
  clean_names() |>
  pivot_longer(cols = where(is.numeric)) |>
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "white") +
  scale_x_log10(labels = label_comma()) +   # ðŸ‘ˆ echte Werte, log-Skala
  facet_wrap(~ name, scales = "free_x") +
  theme_minimal() +
  labs(
    title = "Distribution of Numerical Variables (logarithmic scale)",
    x = "Value",
    y = "Count"
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 25, hjust = 1) 
    )
```

## Nominal Statistics

### Summary of nominal variables (top categories)

```{r}
#| label: "Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

top_n_per_var <- 10 

nominal_summary <- data |>
  clean_names() |>
  select(where(is.factor), price) |>
  pivot_longer(cols = where(is.factor),
               names_to = "Variable",
               values_to = "Category") |>
  group_by(Variable, Category) |>
  summarise(
    Count = n(),
    Percent = round(100 * Count / nrow(data), 2),
    Mean_Price = round(mean(price, na.rm = TRUE), 0),
    .groups = "drop"
  ) |>
  group_by(Variable) |>
  slice_max(order_by = Count, n = top_n_per_var, with_ties = FALSE) |>
  ungroup()

kable(
  nominal_summary,
  caption = paste0(
    "Top ", top_n_per_var,
    " categories per factor variable (counts, share %, and mean price)"
  ),
  digits = 2,
  align = c("l", "l", "r", "r", "r")
)
```

#### Interpretation

**city**: Most listings in Houston, Tucson, Phoenix, and Los Angeles. Prices range widely â€” highest in Los Angeles (\~\$1.9M), lowest around \$250k (Saint Louis).

**state**: California, Texas, and Florida dominate listings (\>30% total). California shows the highest mean price (\~\$1.1M).

**status**: 55% for sale, 45% sold. Active listings are priced higher (\~\$621k vs. \$515k).

**Overall**: Listings cluster in major U.S. cities and states, with strong regional price differences, especially high in California and large metro areas.

### Visualisation of nominal variables (top categories)

```{r}
#| label: "Visualization Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

nominal_summary <- nominal_summary |>
  group_by(Variable) |>
  mutate(Category = forcats::fct_reorder(Category, Count),
         Category = factor(Category, levels = unique(Category))) |>
  ungroup()

# Plot: Facets untereinander, mit eigener x-Skala und y-Skala pro Variable
ggplot(nominal_summary, aes(x = Count, y = Category, fill = Variable)) +
  geom_col(show.legend = FALSE, alpha = 0.8, width = 0.7) +
  facet_wrap(~ Variable, ncol = 1, scales = "free", drop = TRUE) +
  scale_x_continuous(labels = label_comma()) +   #Tausendertrennung, keine 1e+05
  theme_minimal() +
  labs(
    title = "Top Categories per Factor Variable",
    x = "Count",
    y = "Category"
  ) +
  theme(
    panel.spacing.y = unit(1, "lines"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8),
    plot.margin = ggplot2::margin(5, 15, 5, 5)
  )
```

# Bivariate Analysis

### Pairs Plot (all numeric variables)

```{r}
#| label: "Pairs Plot"
#| code-fold: true

set.seed(123)

data_num <- data |>
  janitor::clean_names() |>
  select(where(is.numeric)) |>
  slice_sample(n = 3000) |>
  mutate(across(everything(), log1p)) 

p <- ggpairs(
  data_num,
  progress = FALSE,
  upper = list(continuous = wrap("cor", size = 4, alignPercent = 0.8, stars = TRUE)),
  lower = list(continuous = wrap("points", alpha = 0.3, size = 0.7)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.7))
)

p +
  theme_minimal(base_size = 11) +
  theme(
    strip.text = element_text(size = 8, face = "bold"),
    panel.grid = element_blank(),
    axis.text = element_text(size = 8),
    axis.title = element_text(size = 9),
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5)
  ) +
  labs(title = "Pairs Plot (log-transformiert, n=3000)")
```

#### Interpretation

Very high Correlation (0.8) between Price and Price per Squaremeter: The pricier a house the more you pay per squaremeter, which is logical since the squaremeters arent the only criterion for price.

Also a minor correlations for price are found in the number of beds (0.34), baths (0.58) and the house size (0.549).

The number of beds and baths correlates with the house size.

Interestingly the Price per Squaremeter does not correlate with the house size.

The lot size interestingly does not correlate with any of the other variables.

### Price vs. House Size by Status

```{r}
#| label: "scatter_price_size"
#| tbl-cap: "Price vs. house size by listing status"
#| code-fold: true

# Stichprobe ziehen fÃ¼r Performance
set.seed(123)
sample_data <- data %>% sample_n(50000)

plot_ly(
  sample_data,
  x = ~house_size,
  y = ~price,
  color = ~status,
  type = "scatter",
  mode = "markers",
  alpha = 0.6
) %>%
  plotly::layout(
    title = list(text = "Relationship Between House Size and Price by Status"),
    xaxis = list(title = "House Size (sqft)"),
    yaxis = list(title = "Price ($)", type = "log")
  )
```

#### Interpretation

**Positive relationship**: Larger houses generally have **higher prices**, though the relationship weakens for very large properties.

**Status comparison**: Both for_sale and sold homes follow **similar trends**, but **for_sale** listings appear higher in price, suggesting sellers may list above sale values.

**High variation**: At similar sizes, prices vary widely â€” showing the strong influence of **location** and other factors.

**Outliers**: A few extremely large or expensive properties stretch the scale upward.



### Average Property Price Map

```{r}
#| label: "state_mapping"
#| code-fold: true

valid_states <- tibble(
  state_name = c(state.name, "District of Columbia"),
  state_abbr = c(state.abb,  "DC")
)
```

```{r}
#| label: "map_avg_price"
#| tbl-cap: "Average property price by U.S. state"
#| code-fold: true

map_price <- data |>
  group_by(state) |>
  summarise(avg_price = mean(price, na.rm = TRUE), .groups = "drop") |>
  inner_join(valid_states, by = c("state" = "state_name")) |>
  mutate(avg_price_k = avg_price / 1000)

plot_ly(
  map_price,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_price_k,
  text = ~paste0(state, "<br>Avg Price: $", round(avg_price_k, 1), "K"),
  colorscale = list(c(0, 1), c("lightblue", "darkblue")),
  colorbar = list(title = "Avg Price ($K)")
) |>
  plotly::layout(
    title = list(text = "Average Property Price by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )

```

#### Interpretation

**Regional variation**: Western and coastal states show generally **higher property prices**, while central regions are lower.

**Highest averages**: States like **California, New York, and Washington** stand out with mean prices well above **\$1M**.

**Moderate prices**: States such as Texas, Florida, and Arizona fall in the **mid-range** (\~\$400â€“650K).

**Lower averages**: Midwest and Southern states have more affordable properties on average.

**Summary**: Property values are heavily influenced by **geography** â€” with the highest prices concentrated along the coasts and major urban centers.

### Average House Size Map

```{r}
#| label: "map_avg_size"
#| tbl-cap: "Average house size by U.S. state"
#| code-fold: true

map_size <- data %>%
  group_by(state) %>%
  summarise(avg_size = mean(house_size, na.rm = TRUE), .groups = "drop") %>%
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_size,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_size,
  text = ~paste0(state, "<br>Avg Size: ", round(avg_size), " sqft"),
  colorscale = list(c(0, 1), c("lightgreen", "darkgreen")),
  colorbar = list(title = "Avg Size (sqft)")
) %>%
  plotly::layout(
    title = list(text = "Average House Size by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**General trend**: Average house sizes are fairly consistent across most states, typically around 2,000â€“2,500 sqft.

**Larger homes**: Some central and mountain states (e.g., Colorado, Utah, Iowa) show slightly larger averages, possibly due to more available land.

**Smaller homes**: Coastal and densely populated states (e.g., New York, California) tend to have smaller average house sizes.

### Price Range by US State

```{r}
#| label: "map_extreme_price"
#| tbl-cap: "Price range (max âˆ’ min) by U.S. state"
#| code-fold: true

map_extremes <- data |>
  group_by(state) |>
  summarise(
    min_price = suppressWarnings(min(price, na.rm = TRUE)),
    max_price = suppressWarnings(max(price, na.rm = TRUE)),
    .groups = "drop"
  ) |>
  mutate(range_price = max_price - min_price) |>
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_extremes,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~range_price,
  text = ~paste0(
    state,
    "<br>Min: $", formatC(min_price, big.mark = ",", format = "f", digits = 0),
    "<br>Max: $", formatC(max_price, big.mark = ",", format = "f", digits = 0)
  ),
  colorscale = "Reds",
  colorbar = list(title = "Price Range ($)")
) |>
  plotly::layout(
    title = list(text = "Price Extrem Values by U.S. State (Max âˆ’ Min)"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**Highest ranges**: California shows by far the **largest price range** (over \$400M), driven by extremely high luxury property values.

**Moderate ranges**: States like Florida and parts of the Northeast also show wide **price spreads**, reflecting diverse markets from affordable to luxury homes.

**Lower ranges**: Most central and midwestern states have smaller price gaps, indicating more uniform housing markets.

# Machine Learning Approach

## Data Engineering

```{r}
required_cols <- c(
  "brokered_by","status","price","bed","bath","acre_lot","street",
  "city","state","zip_code","house_size","prev_sold_date"
)

missing_cols <- setdiff(required_cols, names(data))
if (length(missing_cols) > 0) {
  stop("Missing required columns in `data`: ", paste(missing_cols, collapse = ", "))
}

# Ensure basic types
data <- data %>%
  mutate(
    price = as.numeric(price),
    bed = as.numeric(bed),
    bath = as.numeric(bath),
    acre_lot = as.numeric(acre_lot),
    house_size = as.numeric(house_size),
    state = as.factor(state),
    status = as.factor(status),
    zip_code = as.character(zip_code)
  )

# Optional: remove obvious invalids (tune as needed)
data <- data %>%
  filter(
    is.finite(price), price > 0,
    is.finite(house_size), house_size > 0
  )
```

Split the data into trainings data and test data

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

### Leakage-safe K-fold target encoding for zip_code
```{r}
kfold_target_encode_median <- function(train_df, test_df, col, y, k = 5, seed = 123) {
  set.seed(seed)

  # Global median fallback (train only)
  global_median <- median(train_df[[y]], na.rm = TRUE)

  # Create folds
  n <- nrow(train_df)
  fold_id <- sample(rep(seq_len(k), length.out = n))

  # Placeholder for encoded train values
  encoded_train <- rep(NA_real_, n)

  # Precompute full train mapping (for test + fallback)
  full_map <- train_df %>%
    group_by(.data[[col]]) %>%
    summarise(.enc = median(.data[[y]], na.rm = TRUE), .groups = "drop")

  # Fold-wise encoding
  for (fold in seq_len(k)) {
    idx_val <- which(fold_id == fold)
    df_fit  <- train_df[-idx_val, , drop = FALSE]
    df_val  <- train_df[idx_val,  , drop = FALSE]

    fold_map <- df_fit %>%
      group_by(.data[[col]]) %>%
      summarise(.enc = median(.data[[y]], na.rm = TRUE), .groups = "drop")

    enc_val <- df_val %>%
      select(.data[[col]]) %>%
      left_join(fold_map, by = setNames(col, col)) %>%
      mutate(.enc = ifelse(is.na(.enc), global_median, .enc)) %>%
      pull(.enc)

    encoded_train[idx_val] <- enc_val
  }

  # Apply to test using full mapping
  encoded_test <- test_df %>%
    select(.data[[col]]) %>%
    left_join(full_map, by = setNames(col, col)) %>%
    mutate(.enc = ifelse(is.na(.enc), global_median, .enc)) %>%
    pull(.enc)

  list(
    train_enc = encoded_train,
    test_enc  = encoded_test,
    global_median = global_median,
    full_map = full_map
  )
}

# Create log target on TRAIN/TEST (train-only median used for encoding)
train_data <- train_data %>% mutate(log_price = log(price))
test_data  <- test_data  %>% mutate(log_price = log(price))

zip_te <- kfold_target_encode_median(
  train_df = train_data,
  test_df  = test_data,
  col = "zip_code",
  y   = "log_price",   # IMPORTANT: encode using log target for stability
  k   = 5,
  seed = 123
)

train_data <- train_data %>% mutate(zip_te = zip_te$train_enc)
test_data  <- test_data  %>% mutate(zip_te = zip_te$test_enc)
```

### Feature Engineering

```{r}
eps <- 1e-6

feature_engineer <- function(df) {
  df %>%
    mutate(
      # log transforms (handle zero/NA robustly)
      log_house_size = log(pmax(house_size, eps)),
      log_acre_lot   = log(pmax(acre_lot, eps)),

      # ratio features (predictors-only)
      beds_per_sqft  = bed / pmax(house_size, 1),
      baths_per_sqft = bath / pmax(house_size, 1),
      baths_per_bed  = bath / pmax(bed, 1),

      # mild nonlinearities as numeric features
      bed_sq   = bed^2,
      bath_sq  = bath^2
    )
}

train_fe <- feature_engineer(train_data)
test_fe  <- feature_engineer(test_data)

```

### Metrics helper

```{r}
regression_metrics <- function(y_true, y_pred) {
  y_true <- as.numeric(y_true)
  y_pred <- as.numeric(y_pred)

  rmse <- sqrt(mean((y_true - y_pred)^2, na.rm = TRUE))
  mae  <- mean(abs(y_true - y_pred), na.rm = TRUE)

  # R2
  ss_res <- sum((y_true - y_pred)^2, na.rm = TRUE)
  ss_tot <- sum((y_true - mean(y_true, na.rm = TRUE))^2, na.rm = TRUE)
  r2 <- 1 - ss_res / ss_tot

  tibble(RMSE = rmse, MAE = mae, R2 = r2)
}
```

### Linear Regression

```{r}
model_lr_log <- lm(
  log_price ~ log_house_size + log_acre_lot +
    bed + bath + bed_sq + bath_sq +
    beds_per_sqft + baths_per_sqft + baths_per_bed +
    zip_te + state + status,
  data = train_fe
)

lr_log_pred <- predict(model_lr_log, newdata = test_fe)
lr_pred_price <- exp(lr_log_pred)

lm_metrics <- regression_metrics(test_fe$price, lr_pred_price)
lm_metrics
```

### Random Forest

```{r}
# Candidate grid (adjust if runtime is too high)
rf_grid <- expand.grid(
  num.trees     = c(300, 500),
  mtry          = c(2, 3, 4),
  min.node.size = c(5, 10, 20),
  splitrule     = c("variance", "extratrees"),
  stringsAsFactors = FALSE
)

rf_formula <- log_price ~ log_house_size + log_acre_lot +
  bed + bath + bed_sq + bath_sq +
  beds_per_sqft + baths_per_sqft + baths_per_bed +
  zip_te + state + status

set.seed(123)
rf_results <- rf_grid %>%
  mutate(
    rmse_log = pmap_dbl(list(num.trees, mtry, min.node.size, splitrule), function(nt, mt, mns, sr) {
      mod <- ranger(
        formula = rf_formula,
        data = train_fe,
        num.trees = nt,
        mtry = mt,
        min.node.size = mns,
        splitrule = sr,
        importance = "impurity",
        respect.unordered.factors = "order"
      )
      pred_log <- predict(mod, data = test_fe)$predictions
      sqrt(mean((test_fe$log_price - pred_log)^2, na.rm = TRUE))
    })
  ) %>%
  arrange(rmse_log)

rf_results

best_rf <- rf_results %>% slice(1)
best_rf

set.seed(123)
rf_model <- ranger(
  formula = rf_formula,
  data = train_fe,
  num.trees = best_rf$num.trees,
  mtry = best_rf$mtry,
  min.node.size = best_rf$min.node.size,
  splitrule = best_rf$splitrule,
  importance = "impurity",
  respect.unordered.factors = "order"
)

rf_pred_log   <- predict(rf_model, data = test_fe)$predictions
rf_pred_price <- exp(rf_pred_log)

rf_metrics <- regression_metrics(test_fe$price, rf_pred_price)
rf_metrics

# Variable importance
importance_df <- enframe(rf_model$variable.importance, name = "Variable", value = "Importance") %>%
  arrange(desc(Importance))
importance_df
```

### Neureal Network

```{r}
# For NN, keep the same predictors as above, but use model.matrix
# Remove intercept column from x matrix (nnet adds bias internally)
nn_formula <- ~ log_house_size + log_acre_lot +
  bed + bath + bed_sq + bath_sq +
  beds_per_sqft + baths_per_sqft + baths_per_bed +
  zip_te + state + status

x_train <- model.matrix(nn_formula, data = train_fe)[, -1, drop = FALSE]
x_test  <- model.matrix(nn_formula, data = test_fe)[, -1, drop = FALSE]

# Scale using TRAIN stats only
x_center <- colMeans(x_train, na.rm = TRUE)
x_scale  <- apply(x_train, 2, sd, na.rm = TRUE)
x_scale[x_scale == 0 | is.na(x_scale)] <- 1

scale_with <- function(x, center, scale) {
  sweep(sweep(x, 2, center, "-"), 2, scale, "/")
}

x_train_sc <- scale_with(x_train, x_center, x_scale)
x_test_sc  <- scale_with(x_test,  x_center, x_scale)

y_train <- train_fe$log_price
y_test  <- test_fe$log_price

# NN hyperparameter grid (small but meaningful)
nn_grid <- expand.grid(
  size  = c(8, 16, 32),
  decay = c(0.001, 0.01, 0.1),
  stringsAsFactors = FALSE
)

set.seed(123)
nn_results <- nn_grid %>%
  mutate(
    rmse_log = map2_dbl(size, decay, ~ {
      mod <- nnet(
        x = x_train_sc,
        y = y_train,
        size = .x,
        decay = .y,
        linout = TRUE,
        maxit = 1000,
        trace = FALSE
      )
      pred_log <- as.numeric(predict(mod, x_test_sc))
      sqrt(mean((y_test - pred_log)^2, na.rm = TRUE))
    })
  ) %>%
  arrange(rmse_log)

nn_results

best_nn <- nn_results %>% slice(1)
best_nn

set.seed(123)
nn_model <- nnet(
  x = x_train_sc,
  y = y_train,
  size = best_nn$size,
  decay = best_nn$decay,
  linout = TRUE,
  maxit = 1000,
  trace = FALSE
)

nn_pred_log   <- as.numeric(predict(nn_model, x_test_sc))
nn_pred_price <- exp(nn_pred_log)

nn_metrics <- regression_metrics(test_fe$price, nn_pred_price)
nn_metrics

```


## Comparision of the Models

```{r}
bind_rows(
  lm_log = lm_metrics,
  rf_log = rf_metrics,
  nn_log = nn_metrics,
  .id = "model"
)
```


# Conclusion

