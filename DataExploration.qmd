---
title: "Machine Learning Models Gabriel Hamulic"
subtitle: "Dataset: US Real Estate Dataset"
author: "Hamulic, Gabriel"
date: today
embed-resources: true
format:
  html:
    output-file: US-Real-Estate_Gabriel_Hamulic.html
    #output-ext: "html.html" 
    toc: true
    toc-location: right
    code-link: true
    code-tools: true
    #df-print: kable
    theme:
      light: flatly
      dark: darkly
    #echo: fenced
  pdf: 
    output-file: US-Real-Estate_Gabriel_Hamulic.pdf 
    toc: true
    number-sections: true
    code-link: true
    df-print: tibble
    crossref: 
      lof-title: "List of Figures"
fig-align: center
execute: 
  warning: false
---

\listoffigures 
\listoftables
\listoflistings

{{< pagebreak >}}

# Introduction

This project applies and compares several machine learning methods to predict residential property prices. Using a structured housing dataset, the full data science pipeline is implemented, including data preparation, feature engineering, exploratory analysis, and regression modeling. Multiple models â€” including tree-based approaches and a neural network â€” are trained and tuned, and their predictive performance is evaluated on a separate test set using metrics such as RMSE, MAE, and RÂ².

In addition to performance comparison, the project also focuses on model interpretation to understand which features most influence price predictions. The results are summarized in a compact dashboard, providing an overview of model accuracy and key insights. Overall, the project demonstrates how different machine learning techniques can be systematically evaluated in a real-world regression task.

## Libraries

```{r}
#| code-summary: Libraries
#| code-fold: true
library <- function(...) {suppressPackageStartupMessages(base::library(...))}
library(ranger)
library(tidyverse)
library(dplyr)
library(knitr)
library(tidyr)
library(rmarkdown)
library(janitor)
library(scales)
library(tidytext)
library(ggforce)
library(GGally)
library(DT)
library(kableExtra)
library(broom)
library(plotly)
library(nnet)


```

# Data

## Data source

In this data exploration we are looking at the US Real Estate market with use of a dataset from kaggle published by Ahmed Shahriar Sakib. It contains over 2.2 Million Real Estate listings broken down to State, Size, Price (among other factors). (Source: <https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset/data>)

## Data import

```{r setup, include=FALSE}
options(dplyr.print_max = 15, dplyr.print_min = 10)
```

```{r}
data = read.csv("data/realtor-data.zip.csv") # Data import
```

## Data Transformation

```{r}
data = subset(data, select = c(status, price, bed, bath, acre_lot, city, state, house_size)) # keep relevant columns
```

### Structure with standard datatypes

```{r}
#| label: "data_structure"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) 
      as.character(val)
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

```{r}
#| code-fold: true

# Assign Data Types
data$status = as.factor(data$status)
data$city = as.factor(data$city)
data$state = as.factor(data$state)
```

### NA Removal

```{r}
#| code-fold: true
before_rows <- nrow(data)
data <- na.omit(data)
after_rows <- nrow(data)
kable(data.frame(Description = c("Before NA removal", "After NA removal"),
Rows = c(before_rows, after_rows)))
```

The dataset now has `r nrow(data)` observations and `r ncol(data)` variables after removing rows with missing values.

### Filtering

```{r}
#| code-fold: true

# Filter min and max values
data = data |>
  filter(price > 10000 & price < 1000000000)
```

### Calculations

```{r}
#| code-fold: true

data = data |>
  mutate(price_per_sqm = price/house_size)
```

### Cleaned Dataset

```{r}
#| code-fold: true

paged_table(data)
```

### Structure after transformation

```{r}
#| label: "data_structure_after_transformation"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) as.character(val) else as.character(round(val, 2))
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

After cleaning the dataset, all variables have appropriate data types and no missing values (**n = `r nrow(data)`**):

-   **status** â€“ Factor variable showing the listing status (e.g., for_sale, sold).

-   **price** â€“ Numeric value for the propertyâ€™s price in USD.

-   **bed, bath** â€“ Integer counts of bedrooms and bathrooms.

-   **acre_lot** â€“ Numeric size of the lot (in acres).

-   **city, state** â€“ Factor variables identifying the propertyâ€™s location.

-   **house_size** â€“ Numeric size of the house (in square feet).

-   **price_per_sqm** â€“ Numeric variable derived from price / house_size to compare prices across properties.

All rows with missing data were removed, and categorical variables were converted to factors for easier analysis and visualization later on.

## Data dictionary

```{r}
#| code-fold: true

tibble(
  Variable = c("price", "status", "acre_lot", "state", "house_size", "price_per_sqm"),
  Description = c(
    "The price for which the item was listed on the market",
    "The status if the house is already sold or still for sale",
    "The size of the land / lot on which the house is located in acres",
    "The state in which the house is located",
    "The size of the house in square feet",
    "The price per square footage"
  )
) |>
  kable(
    caption = "Description of key variables in the dataset",
    align = c("l", "l")
  )
```

# Summary statistic tables

In this section we will cover the summary of our cleaned dataset. We will explore basic statistical values from our data.

## Numeric Statistics

### Summary of numerical values

```{r}
#| label: "Numeric Statistics"
#| tbl-cap: "Summary statistics of numerical variables in dataframe"
#| code-fold: true
data |> 
  janitor::clean_names() |>
  mutate(row = row_number() |> factor()) |> 
  pivot_longer(cols = where(is.numeric)) |> 
  group_by(name) |> 
  summarize(N = n(),
            min = min(value),
            mean = mean(value),
            median = median(value),
            max = max(value),
            st.dev = sd(value)
            ) |> 
  knitr::kable(digits = 2)
```

#### Interpretation

**price**: Very wide range (\$10.4kâ€“\$51.5M). Mean (\$573k), median (\$379k), indicating strong right-skew and high-priced outliers.

**house_size**: Average \~2,119 sqft, median 1,812 sqft. Extremely large max (1,560,780 sqft) signals outliers. The distribution is right-skewed.

**acre_lot**: Median 0.21 acres vs. mean 12.75 acres â†’ a few very large parcels inflate the mean.

**bed / bath**: Typical homes (\~3 beds, 2 baths) with modest spread; minima at 1 suggest realistic counts.

**price_per_sqm**: Mean \$262.42 vs. median \$197.42, also right-skewed, consistent with price outliers.

### Visualisation of numerical values

```{r}
#| label: "Logarithmic Visualisation"
#| tbl-cap: "Visualisation of numerical variables in dataframe"
#| code-fold: true
data |>
  clean_names() |>
  pivot_longer(cols = where(is.numeric)) |>
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "white") +
  scale_x_log10(labels = label_comma()) +   # ðŸ‘ˆ echte Werte, log-Skala
  facet_wrap(~ name, scales = "free_x") +
  theme_minimal() +
  labs(
    title = "Distribution of Numerical Variables (logarithmic scale)",
    x = "Value",
    y = "Count"
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 25, hjust = 1) 
    )
```

## Nominal Statistics

### Summary of nominal variables (top categories)

```{r}
#| label: "Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

top_n_per_var <- 10 

nominal_summary <- data |>
  clean_names() |>
  select(where(is.factor), price) |>
  pivot_longer(cols = where(is.factor),
               names_to = "Variable",
               values_to = "Category") |>
  group_by(Variable, Category) |>
  summarise(
    Count = n(),
    Percent = round(100 * Count / nrow(data), 2),
    Mean_Price = round(mean(price, na.rm = TRUE), 0),
    .groups = "drop"
  ) |>
  group_by(Variable) |>
  slice_max(order_by = Count, n = top_n_per_var, with_ties = FALSE) |>
  ungroup()

kable(
  nominal_summary,
  caption = paste0(
    "Top ", top_n_per_var,
    " categories per factor variable (counts, share %, and mean price)"
  ),
  digits = 2,
  align = c("l", "l", "r", "r", "r")
)
```

#### Interpretation

**city**: Most listings in Houston, Tucson, Phoenix, and Los Angeles. Prices range widely â€” highest in Los Angeles (\~\$1.9M), lowest around \$250k (Saint Louis).

**state**: California, Texas, and Florida dominate listings (\>30% total). California shows the highest mean price (\~\$1.1M).

**status**: 55% for sale, 45% sold. Active listings are priced higher (\~\$621k vs. \$515k).

**Overall**: Listings cluster in major U.S. cities and states, with strong regional price differences, especially high in California and large metro areas.

### Visualisation of nominal variables (top categories)

```{r}
#| label: "Visualization Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

nominal_summary <- nominal_summary |>
  group_by(Variable) |>
  mutate(Category = forcats::fct_reorder(Category, Count),
         Category = factor(Category, levels = unique(Category))) |>
  ungroup()

# Plot: Facets untereinander, mit eigener x-Skala und y-Skala pro Variable
ggplot(nominal_summary, aes(x = Count, y = Category, fill = Variable)) +
  geom_col(show.legend = FALSE, alpha = 0.8, width = 0.7) +
  facet_wrap(~ Variable, ncol = 1, scales = "free", drop = TRUE) +
  scale_x_continuous(labels = label_comma()) +   #Tausendertrennung, keine 1e+05
  theme_minimal() +
  labs(
    title = "Top Categories per Factor Variable",
    x = "Count",
    y = "Category"
  ) +
  theme(
    panel.spacing.y = unit(1, "lines"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8),
    plot.margin = ggplot2::margin(5, 15, 5, 5)
  )
```

# Bivariate Analysis

### Pairs Plot (all numeric variables)

```{r}
#| label: "Pairs Plot"
#| code-fold: true

set.seed(123)

data_num <- data |>
  janitor::clean_names() |>
  select(where(is.numeric)) |>
  slice_sample(n = 3000) |>
  mutate(across(everything(), log1p)) 

p <- ggpairs(
  data_num,
  progress = FALSE,
  upper = list(continuous = wrap("cor", size = 4, alignPercent = 0.8, stars = TRUE)),
  lower = list(continuous = wrap("points", alpha = 0.3, size = 0.7)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.7))
)

p +
  theme_minimal(base_size = 11) +
  theme(
    strip.text = element_text(size = 8, face = "bold"),
    panel.grid = element_blank(),
    axis.text = element_text(size = 8),
    axis.title = element_text(size = 9),
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5)
  ) +
  labs(title = "Pairs Plot (log-transformiert, n=3000)")
```

#### Interpretation

Very high Correlation (0.8) between Price and Price per Squaremeter: The pricier a house the more you pay per squaremeter, which is logical since the squaremeters arent the only criterion for price.

Also a minor correlations for price are found in the number of beds (0.34), baths (0.58) and the house size (0.549).

The number of beds and baths correlates with the house size.

Interestingly the Price per Squaremeter does not correlate with the house size.

The lot size interestingly does not correlate with any of the other variables.

### Price vs. House Size by Status

```{r}
#| label: "scatter_price_size"
#| tbl-cap: "Price vs. house size by listing status"
#| code-fold: true

# Stichprobe ziehen fÃ¼r Performance
set.seed(123)
sample_data <- data %>% sample_n(50000)

plot_ly(
  sample_data,
  x = ~house_size,
  y = ~price,
  color = ~status,
  type = "scatter",
  mode = "markers",
  alpha = 0.6
) %>%
  plotly::layout(
    title = list(text = "Relationship Between House Size and Price by Status"),
    xaxis = list(title = "House Size (sqft)"),
    yaxis = list(title = "Price ($)", type = "log")
  )
```

#### Interpretation

**Positive relationship**: Larger houses generally have **higher prices**, though the relationship weakens for very large properties.

**Status comparison**: Both for_sale and sold homes follow **similar trends**, but **for_sale** listings appear higher in price, suggesting sellers may list above sale values.

**High variation**: At similar sizes, prices vary widely â€” showing the strong influence of **location** and other factors.

**Outliers**: A few extremely large or expensive properties stretch the scale upward.



### Average Property Price Map

```{r}
#| label: "state_mapping"
#| code-fold: true

valid_states <- tibble(
  state_name = c(state.name, "District of Columbia"),
  state_abbr = c(state.abb,  "DC")
)
```

```{r}
#| label: "map_avg_price"
#| tbl-cap: "Average property price by U.S. state"
#| code-fold: true

map_price <- data |>
  group_by(state) |>
  summarise(avg_price = mean(price, na.rm = TRUE), .groups = "drop") |>
  inner_join(valid_states, by = c("state" = "state_name")) |>
  mutate(avg_price_k = avg_price / 1000)

plot_ly(
  map_price,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_price_k,
  text = ~paste0(state, "<br>Avg Price: $", round(avg_price_k, 1), "K"),
  colorscale = list(c(0, 1), c("lightblue", "darkblue")),
  colorbar = list(title = "Avg Price ($K)")
) |>
  plotly::layout(
    title = list(text = "Average Property Price by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )

```

#### Interpretation

**Regional variation**: Western and coastal states show generally **higher property prices**, while central regions are lower.

**Highest averages**: States like **California, New York, and Washington** stand out with mean prices well above **\$1M**.

**Moderate prices**: States such as Texas, Florida, and Arizona fall in the **mid-range** (\~\$400â€“650K).

**Lower averages**: Midwest and Southern states have more affordable properties on average.

**Summary**: Property values are heavily influenced by **geography** â€” with the highest prices concentrated along the coasts and major urban centers.

### Average House Size Map

```{r}
#| label: "map_avg_size"
#| tbl-cap: "Average house size by U.S. state"
#| code-fold: true

map_size <- data %>%
  group_by(state) %>%
  summarise(avg_size = mean(house_size, na.rm = TRUE), .groups = "drop") %>%
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_size,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_size,
  text = ~paste0(state, "<br>Avg Size: ", round(avg_size), " sqft"),
  colorscale = list(c(0, 1), c("lightgreen", "darkgreen")),
  colorbar = list(title = "Avg Size (sqft)")
) %>%
  plotly::layout(
    title = list(text = "Average House Size by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**General trend**: Average house sizes are fairly consistent across most states, typically around 2,000â€“2,500 sqft.

**Larger homes**: Some central and mountain states (e.g., Colorado, Utah, Iowa) show slightly larger averages, possibly due to more available land.

**Smaller homes**: Coastal and densely populated states (e.g., New York, California) tend to have smaller average house sizes.

### Price Range by US State

```{r}
#| label: "map_extreme_price"
#| tbl-cap: "Price range (max âˆ’ min) by U.S. state"
#| code-fold: true

map_extremes <- data |>
  group_by(state) |>
  summarise(
    min_price = suppressWarnings(min(price, na.rm = TRUE)),
    max_price = suppressWarnings(max(price, na.rm = TRUE)),
    .groups = "drop"
  ) |>
  mutate(range_price = max_price - min_price) |>
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_extremes,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~range_price,
  text = ~paste0(
    state,
    "<br>Min: $", formatC(min_price, big.mark = ",", format = "f", digits = 0),
    "<br>Max: $", formatC(max_price, big.mark = ",", format = "f", digits = 0)
  ),
  colorscale = "Reds",
  colorbar = list(title = "Price Range ($)")
) |>
  plotly::layout(
    title = list(text = "Price Extrem Values by U.S. State (Max âˆ’ Min)"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**Highest ranges**: California shows by far the **largest price range** (over \$400M), driven by extremely high luxury property values.

**Moderate ranges**: States like Florida and parts of the Northeast also show wide **price spreads**, reflecting diverse markets from affordable to luxury homes.

**Lower ranges**: Most central and midwestern states have smaller price gaps, indicating more uniform housing markets.

# Data Engineering

Split the data into trainings data and test data

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```


### Linear Regression

```{r}
#| code-fold: true

model_lr <- lm(price ~ house_size + bath + bed + state + status, data = train_data)
tidy(model_lr) |>
  arrange(p.value) |>
  mutate(
    estimate = round(estimate, 1),
    std.error = round(std.error, 1),
    statistic = round(statistic, 1),
    p.value = signif(p.value, 3)
  ) |>
  datatable(
    caption = "Regressionsergebnisse (interaktiv)",
    filter = "top",         
    options = list(
      pageLength = 10,      
      autoWidth = TRUE,
      responsive = TRUE
    )
  )
```

```{r}
#| label: "Graphic Regression"
#| code-fold: true
 
tidy(model_lr, conf.int = TRUE) |>
  filter(term != "(Intercept)") |>
  mutate(term = reorder(term, estimate)) |>
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Greatest contributors to price",
    x = "regression coefficient",
    y = ""
  )
```

The cities are exempt from this regression as the number of distinct variables would be too large for the model. Therefore for location only states will be used.

Locations like US insular areas like Hawaii and the Virgin Islands appear to be the greatest contributors to price, California being third.

Regarding the numeric variables the number of baths increases housing price the most with around 384.000\$ per bath room. Every square meter of the house size adds 16.7\$ to the price, each acre of the lot around 14.2\$. Interestingly each additional bed appears to decrease the price by around 81.946\$. At this point it has to be noted, that these price changes assume that for each variable all other variables would be constant.

### Interpretation of Linear Regression

```{r}
# Vorhersagen auf Testdaten
lm_preds <- predict(model_lr, newdata = test_data)

# Kennwerte berechnen
lm_metrics <- tibble(
  RMSE = sqrt(mean((test_data$price - lm_preds)^2)),
  MAE  = mean(abs(test_data$price - lm_preds)),
  R2   = 1 - sum((test_data$price - lm_preds)^2) /
              sum((test_data$price - mean(test_data$price))^2)
)

lm_metrics
```

The linear regression model achieves a test RMSE of approximately 1,018,695, which is higher than the Random Forest but lower than the neural network. This indicates that while the model captures some systematic relationship between predictors and price, its linear structure cannot fully model the complexity of the housing market. The result suggests moderate predictive ability but clear underfitting compared to more flexible models.

### Hyperparameter for Random Forest

```{verbatim}
set.seed(123)

# einfache Grid-Suche
rf_grid <- expand.grid(
  num.trees = c(100, 300, 500),
  mtry      = c(2, 3, 4)
)

rf_results <- rf_grid |>
  mutate(
    rmse = map2_dbl(num.trees, mtry, ~ {
      model <- ranger(
        price ~ house_size + bed + bath + acre_lot + state + status,
        data = train_data,
        num.trees = .x,
        mtry = .y,
        importance = "impurity"
      )
      preds <- predict(model, data = test_data)$predictions
      sqrt(mean((test_data$price - preds)^2))
    })
  ) |>
  arrange(rmse)

rf_results
```

### Interpretation for Hyperparameters of Random Forest

Before interpreting the results, it is useful to briefly clarify the meaning of the two Random Forest hyperparameters:

**num.trees**
Specifies the number of decision trees grown in the forest.
Increasing num.trees generally reduces variance and stabilizes predictions, but beyond a certain point the performance gains become marginal while computational cost increases.

**mtry**
Defines the number of predictor variables randomly selected as candidates at each split.
Smaller values increase randomness and tree diversity, which can reduce overfitting, whereas larger values make individual trees more similar and may increase variance.

  num.trees mtry     rmse
1       300    2 838263.1
2       500    2 839089.1
3       100    2 846528.8
4       500    3 860607.9
5       100    3 863708.7
6       300    3 867879.5
7       100    4 889648.8
8       300    4 896180.3
9       500    4 902455.2

The grid search results show clear performance differences across combinations of num.trees and mtry:

The best-performing configuration is
**num.trees = 300** and **mtry = 2**, achieving the lowest **RMSE (838,263.1).**

Increasing the number of trees from 300 to 500 with the same mtry = 2 does not improve performance (RMSE slightly increases to 839,089.1), indicating diminishing returns from adding more trees.

Using only 100 trees leads to a noticeably higher RMSE (846,528.8), suggesting that the forest is not yet sufficiently stable.

Higher values of mtry (3 or 4) consistently result in worse RMSE, regardless of the number of trees. This indicates that allowing too many variables at each split reduces tree diversity and increases overfitting.

## Random Forest

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)
rf_model <- ranger(
  formula = price ~ house_size + bed + bath + acre_lot + state + status,
  data = train_data,
  importance = "impurity",
  num.trees = 300,
  mtry = 2
)

importance_df <- enframe(rf_model$variable.importance, name = "Variable", value = "Importance") %>%
  arrange(desc(Importance))

ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance, fill = Importance)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Variable Importance from Random Forest Model",
    x = "Variable",
    y = "Importance"
  )

```

### Interpretation of Random Forest Model

```{r}
# Vorhersagen auf Testdaten
rf_preds <- predict(rf_model, data = test_data)$predictions

# Kennwerte berechnen
rf_metrics <- tibble(
  RMSE = sqrt(mean((test_data$price - rf_preds)^2)),
  MAE  = mean(abs(test_data$price - rf_preds)),
  R2   = 1 - sum((test_data$price - rf_preds)^2) /
              sum((test_data$price - mean(test_data$price))^2)
)

rf_metrics
```

The Random Forest shows the best performance with a test RMSE of about 842,861, which is a substantial improvement over both linear regression and the neural network. This reduction in error indicates that modeling non-linearities and interactions between variables significantly improves predictions. The model therefore captures the structure of the data more effectively than the other approaches.

### Hyperparameter for Neural Network

```{verbatim}
nn_grid <- expand.grid(
  size  = c(3, 5, 7),   # Anzahl Hidden Neurons
  decay = c(0, 0.01, 0.1)
)

nn_results <- nn_grid |>
  mutate(
    rmse = map2_dbl(size, decay, ~ {
      model <- nnet(
        price ~ house_size + bed + bath + acre_lot + state + status,
        data = train_data,
        size = .x,
        decay = .y,
        linout = TRUE,
        maxit = 500,
        trace = FALSE
      )
      preds <- predict(model, test_data)
      sqrt(mean((test_data$price - preds)^2))
    })
  ) |>
  arrange(rmse)

nn_results
```

### Interpretation for Hyperparameters

Before interpreting the results, it is useful to briefly clarify the meaning of the two neural network hyperparameters:

**size**
Specifies the number of neurons in the hidden layer.
A larger size increases the modelâ€™s capacity to learn complex, non-linear relationships, but also raises the risk of overfitting and increases computational complexity.

**decay**
Controls the strength of weight decay (L2 regularization).
Higher values of decay penalize large weights more strongly, which can improve generalization by reducing overfitting, while a value of zero corresponds to no regularization.

  size decay    rmse
1    7  0.10 1100793
2    7  0.00 1141877
3    7  0.01 1141878
4    3  0.00 1141878
5    5  0.01 1141878
6    3  0.10 1141878
7    3  0.01 1141878
8    5  0.00 1141878
9    5  0.10 1141878

The grid search results indicate limited sensitivity of model performance to most combinations of size and decay, with one notable exception.
The best-performing configuration is **size = 7** and **decay = 0.10**, achieving the **lowest RMSE (1,100,793)**.
This suggests that a larger hidden layer combined with relatively strong regularization allows the neural network to capture more complex relationships in the data while mitigating overfitting.

## Neural Network

```{r}
#| code-fold: true
target   <- "price"

num_vars <- c("house_size", "bed", "bath", "acre_lot")
cat_vars <- c("state", "status")
```

Defines the target and the predictor groups used throughout the NN pipeline. Keeping them in one place makes the feature set easy to change.

```{r}
#| code-fold: true
#| warning: false
#| message: false

library(rsample)

set.seed(123)

data_model <- data[, c(target, num_vars, cat_vars)]

split1 <- initial_split(data_model, prop = 0.7)
train_data <- training(split1)
temp_data  <- testing(split1)

split2 <- initial_split(temp_data, prop = 0.5)
valid_data <- training(split2)
test_data  <- testing(split2)
```
Splits the dataset into train (70%), validation (15%), and test (15%) sets with a fixed seed. Validation is used for tuning/model selection; test is reserved for the final evaluation.

```{r}
#| code-fold: true
#| warning: false
#| message: false

library(recipes)

# Optional: Zielvariable transformieren (empfohlen bei Preisen)
use_log_target <- TRUE

train_nn <- train_data
valid_nn <- valid_data
test_nn  <- test_data

if (use_log_target) {
  train_nn$y <- log1p(train_nn$price)
  valid_nn$y <- log1p(valid_nn$price)
  test_nn$y  <- log1p(test_nn$price)
  outcome <- "y"
} else {
  outcome <- "price"
}

rec <- recipe(as.formula(paste0(outcome, " ~ ", paste(c(num_vars, cat_vars), collapse = " + "))),
              data = train_nn) |>
  step_novel(all_nominal_predictors()) |>               # neue Levels in Valid/Test
  step_other(all_nominal_predictors(), threshold = 0.01) |> # seltene Levels -> "other" (anpassbar)
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())              # z-Score Skalierung

prep_rec <- prep(rec, training = train_nn, retain = TRUE)

train_b <- bake(prep_rec, new_data = train_nn)
valid_b <- bake(prep_rec, new_data = valid_nn)
test_b  <- bake(prep_rec, new_data = test_nn)

x_train <- as.matrix(subset(train_b, select = -all_of(outcome)))
y_train <- train_b[[outcome]]

x_valid <- as.matrix(subset(valid_b, select = -all_of(outcome)))
y_valid <- valid_b[[outcome]]

x_test  <- as.matrix(subset(test_b,  select = -all_of(outcome)))
y_test  <- test_b[[outcome]]

```

Builds a preprocessing recipe for the neural network: optionally log-transforms the target (log1p(price)), handles unseen/rare factor levels, one-hot encodes categorical variables, removes zero-variance predictors, and standardizes numeric predictors (z-scores). The recipe is fitted on the training set and applied to train/validation/test to create x_* matrices and y_* vectors.

```{r}
#| code-fold: true
#| warning: false
#| message: false

library(nnet)
library(tibble)

rmse <- function(truth, pred) sqrt(mean((truth - pred)^2, na.rm = TRUE))
mae  <- function(truth, pred) mean(abs(truth - pred), na.rm = TRUE)
r2   <- function(truth, pred) {
  ss_res <- sum((truth - pred)^2, na.rm = TRUE)
  ss_tot <- sum((truth - mean(truth, na.rm = TRUE))^2, na.rm = TRUE)
  1 - ss_res / ss_tot
}

set.seed(123)

nn_model <- nnet(
  x = x_train,
  y = y_train,
  size   = 7,
  decay  = 0.10,
  linout = TRUE,
  maxit  = 500,
  trace  = FALSE,
  MaxNWts = 20000
)

pred_valid_trf <- as.numeric(predict(nn_model, x_valid))

# ZurÃ¼ck in Original-Skala, falls log1p genutzt wurde
if (use_log_target) {
  pred_valid <- expm1(pred_valid_trf)
  truth_valid <- valid_data$price
} else {
  pred_valid <- pred_valid_trf
  truth_valid <- y_valid
}

tibble(
  set  = "valid",
  RMSE = rmse(truth_valid, pred_valid),
  MAE  = mae(truth_valid, pred_valid),
  R2   = r2(truth_valid, pred_valid)
)
```

Trains an nnet regression model with size = 7 and decay = 0.10, then predicts on the validation set. If a log target is used, predictions are back-transformed with expm1(). Outputs RMSE, MAE, and RÂ² on the original price scale.

```{r}
#| code-fold: true
pred_test_trf <- as.numeric(predict(nn_model, x_test))

if (use_log_target) {
  pred_test <- expm1(pred_test_trf)
  truth_test <- test_data$price
} else {
  pred_test <- pred_test_trf
  truth_test <- y_test
}

tibble(
  set  = "test",
  RMSE = rmse(truth_test, pred_test),
  MAE  = mae(truth_test, pred_test),
  R2   = r2(truth_test, pred_test)
)
```

Generates test predictions (with the same back-transformation to the original scale) and reports final generalization performance on the held-out test set: RMSE = 817,135, MAE = 221,255, RÂ² = 0.423.

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)

# Fit
nn_model <- nnet::nnet(
  price ~ house_size + bed + bath + acre_lot + state + status,
  data   = train_data,
  size   = 7,
  decay  = 0.10,
  linout = TRUE,
  maxit  = 500,
  trace  = FALSE
)

# Predict on test set
nn_preds <- predict(nn_model, newdata = test_data, type = "raw")

# Metrics
nn_rmse <- sqrt(mean((test_data$price - nn_preds)^2, na.rm = TRUE))
nn_mae  <- mean(abs(test_data$price - nn_preds), na.rm = TRUE)
nn_r2   <- 1 - sum((test_data$price - nn_preds)^2, na.rm = TRUE) /
  sum((test_data$price - mean(test_data$price, na.rm = TRUE))^2, na.rm = TRUE)

tibble::tibble(
  model = "nnet",
  rmse  = nn_rmse,
  mae   = nn_mae,
  r2    = nn_r2
)
```

The best tuned neural network configuration (size = 7, decay = 0.10) achieves a test RMSE of roughly 1,100,793, which is the highest error among the three models. Despite its theoretical flexibility, the neural network does not generalize as well in this tabular dataset. This suggests that, in this case, increased model complexity does not automatically translate into better performance.

### Interpretation of Neural Network

```{r}
#| code-fold: true
#| warning: false
#| message: false

# Quick interpretation helpers

# 1) Observed vs Predicted (scatter)
plot(test_data$price, nn_preds,
     xlab = "Observed price", ylab = "Predicted price")
abline(0, 1)

# 2) Permutation importance (simple + model-agnostic)
perm_rmse <- function(df, var) {
  df_perm <- df
  df_perm[[var]] <- sample(df_perm[[var]])
  p <- predict(nn_model, newdata = df_perm, type = "raw")
  sqrt(mean((test_data$price - p)^2, na.rm = TRUE))
}

vars <- c("house_size","bed","bath","acre_lot","state","status")
base_rmse <- nn_rmse

nn_perm_imp <- purrr::map_dbl(vars, ~ perm_rmse(test_data, .x) - base_rmse) |>
  tibble::tibble(variable = vars, rmse_increase = _) |>
  dplyr::arrange(dplyr::desc(rmse_increase))

nn_perm_imp

```

The observed vs. predicted plot reveals that the neural network produces nearly constant predictions across the entire price range. This indicates that the model failed to learn meaningful relationships and effectively underfits the data. The most likely reason is the lack of feature scaling combined with strong regularization, which prevents the network from adjusting its weights appropriately. As a result, the neural network defaults to predicting values close to the overall mean price.

## Comparision of the Models

In terms of RMSE, the ranking is:
Random Forest (â‰ˆ 842k) < Linear Regression (â‰ˆ 1,019k) < Neural Network (â‰ˆ 1,101k).
The Random Forest reduces prediction error by roughly 17% compared to linear regression and by about 24% compared to the neural network, showing a clear performance advantage.

# Conclusion

The results show that model flexibility matters, but the type of flexibility is crucial. Tree-based ensemble methods like Random Forest handle structured, interaction-heavy data best, leading to the lowest prediction error. Linear regression serves as a reasonable baseline, while the neural network does not provide benefits in this setting.