---
title: "Data Exploration Gabriel Hamulic"
subtitle: "Dataset: US Real Estate Dataset"
author: "Hamulic, Gabriel"
date: today
embed-resources: true
format:
  html:
    output-file: US-Real-Estate_Gabriel_Hamulic.html
    #output-ext: "html.html" 
    toc: true
    toc-location: right
    code-link: true
    code-tools: true
    #df-print: kable
    theme:
      light: flatly
      dark: darkly
    #echo: fenced
  pdf: 
    output-file: US-Real-Estate_Gabriel_Hamulic.pdf 
    toc: true
    number-sections: true
    code-link: true
    df-print: tibble
    crossref: 
      lof-title: "List of Figures"
fig-align: center
execute: 
  warning: false
---

\listoffigures 
\listoftables
\listoflistings

::: callout-caution
### Instructions

Your report must be of high quality, meaning that your report:

-   is visually and textually pleasing of
-   does not look/read/feel like a draft instead of a finished analysis
-   explains/discusses your findings and results in the main text, e.g., explain/discuss all figures/table in the main text
-   is representable such that it can show to any interested third party
-   uses figure/table captions/linking/reference (see example further down)
-   Do not show any standard printout of R-code, use for data.frame/tibbles `knitr::kable()` printing.
-   Do not simply print datasets (too many lines) use instead `rmarkdown::paged_table()`
:::

{{< pagebreak >}}

# Introduction

In this project, we explore a large dataset of over 2 million real estate listings across the United States. The data includes key property attributes such as price, size, lot area, location, and listing status.

Our goal is to gain a deeper understanding of the current U.S. housing market by identifying:

-   Patterns in property prices and sizes across states and cities

-   Relationships between housing characteristics (e.g., size, bedrooms, bathrooms)

-   Regional differences in affordability and market activity

## Libraries

```{r}
#| code-summary: Libraries
#| code-fold: true
library <- function(...) {suppressPackageStartupMessages(base::library(...))}
library(ranger)
library(tidyverse)
library(dplyr)
library(knitr)
library(tidyr)
library(rmarkdown)
library(janitor)
library(scales)
library(tidytext)
library(ggforce)
library(GGally)
library(DT)
library(kableExtra)
library(broom)
library(plotly)
library(nnet)


```

# Data

## Data source

In this data exploration we are looking at the US Real Estate market with use of a dataset from kaggle published by Ahmed Shahriar Sakib. It contains over 2.2 Million Real Estate listings broken down to State, Size, Price (among other factors). (Source: <https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset/data>)

## Data import

```{r setup, include=FALSE}
options(dplyr.print_max = 15, dplyr.print_min = 10)
```

```{r}
data = read.csv("data/realtor-data.zip.csv") # Data import
```

## Data Transformation

```{r}
data = subset(data, select = c(status, price, bed, bath, acre_lot, city, state, house_size)) # keep relevant columns
```

### Structure with standard datatypes

```{r}
#| label: "data_structure"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) 
      as.character(val)
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

```{r}
#| code-fold: true

# Assign Data Types
data$status = as.factor(data$status)
data$city = as.factor(data$city)
data$state = as.factor(data$state)
```

### NA Removal

```{r}
#| code-fold: true
before_rows <- nrow(data)
data <- na.omit(data)
after_rows <- nrow(data)
kable(data.frame(Description = c("Before NA removal", "After NA removal"),
Rows = c(before_rows, after_rows)))
```

The dataset now has `r nrow(data)` observations and `r ncol(data)` variables after removing rows with missing values.

### Filtering

```{r}
#| code-fold: true

# Filter min and max values
data = data |>
  filter(price > 10000 & price < 1000000000)
```

### Calculations

```{r}
#| code-fold: true

data = data |>
  mutate(price_per_sqm = price/house_size)
```

### Cleaned Dataset

```{r}
#| code-fold: true

paged_table(data)
```

### Structure after transformation

```{r}
#| label: "data_structure_after_transformation"
#| tbl-cap: "Structure of cleaned dataset"
#| code-fold: true

structure_tbl <- tibble::tibble(
  Variable = names(data),
  Type = sapply(data, function(x) class(x)[1]),
  Example = sapply(data, function(x) {
    val <- unique(x[!is.na(x)])[1]
    if (is.factor(val)) as.character(val) else as.character(round(val, 2))
  }),
  Missing = sapply(data, function(x) sum(is.na(x)))
)

kable(
  structure_tbl,
  caption = "Structure summary of the dataset",
  align = c("l", "l", "l", "r")
)
```

After cleaning the dataset, all variables have appropriate data types and no missing values (**n = `r nrow(data)`**):

-   **status** â€“ Factor variable showing the listing status (e.g., for_sale, sold).

-   **price** â€“ Numeric value for the propertyâ€™s price in USD.

-   **bed, bath** â€“ Integer counts of bedrooms and bathrooms.

-   **acre_lot** â€“ Numeric size of the lot (in acres).

-   **city, state** â€“ Factor variables identifying the propertyâ€™s location.

-   **house_size** â€“ Numeric size of the house (in square feet).

-   **price_per_sqm** â€“ Numeric variable derived from price / house_size to compare prices across properties.

All rows with missing data were removed, and categorical variables were converted to factors for easier analysis and visualization later on.

## Data dictionary

```{r}
#| code-fold: true

tibble(
  Variable = c("price", "status", "acre_lot", "state", "house_size", "price_per_sqm"),
  Description = c(
    "The price for which the item was listed on the market",
    "The status if the house is already sold or still for sale",
    "The size of the land / lot on which the house is located in acres",
    "The state in which the house is located",
    "The size of the house in square feet",
    "The price per square footage"
  )
) |>
  kable(
    caption = "Description of key variables in the dataset",
    align = c("l", "l")
  )
```

# Summary statistic tables

In this section we will cover the summary of our cleaned dataset. We will explore basic statistical values from our data.

## Numeric Statistics

### Summary of numerical values

```{r}
#| label: "Numeric Statistics"
#| tbl-cap: "Summary statistics of numerical variables in dataframe"
#| code-fold: true
data |> 
  janitor::clean_names() |>
  mutate(row = row_number() |> factor()) |> 
  pivot_longer(cols = where(is.numeric)) |> 
  group_by(name) |> 
  summarize(N = n(),
            min = min(value),
            mean = mean(value),
            median = median(value),
            max = max(value),
            st.dev = sd(value)
            ) |> 
  knitr::kable(digits = 2)
```

#### Interpretation

**price**: Very wide range (\$10.4kâ€“\$51.5M). Mean (\$573k), median (\$379k), indicating strong right-skew and high-priced outliers.

**house_size**: Average \~2,119 sqft, median 1,812 sqft. Extremely large max (1,560,780 sqft) signals outliers. The distribution is right-skewed.

**acre_lot**: Median 0.21 acres vs. mean 12.75 acres â†’ a few very large parcels inflate the mean.

**bed / bath**: Typical homes (\~3 beds, 2 baths) with modest spread; minima at 1 suggest realistic counts.

**price_per_sqm**: Mean \$262.42 vs. median \$197.42, also right-skewed, consistent with price outliers.

### Visualisation of numerical values

```{r}
#| label: "Logarithmic Visualisation"
#| tbl-cap: "Visualisation of numerical variables in dataframe"
#| code-fold: true
data |>
  clean_names() |>
  pivot_longer(cols = where(is.numeric)) |>
  ggplot(aes(x = value, fill = name)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "white") +
  scale_x_log10(labels = label_comma()) +   # ðŸ‘ˆ echte Werte, log-Skala
  facet_wrap(~ name, scales = "free_x") +
  theme_minimal() +
  labs(
    title = "Distribution of Numerical Variables (logarithmic scale)",
    x = "Value",
    y = "Count"
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 25, hjust = 1) 
    )
```

## Nominal Statistics

### Summary of nominal variables (top categories)

```{r}
#| label: "Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

top_n_per_var <- 10 

nominal_summary <- data |>
  clean_names() |>
  select(where(is.factor), price) |>
  pivot_longer(cols = where(is.factor),
               names_to = "Variable",
               values_to = "Category") |>
  group_by(Variable, Category) |>
  summarise(
    Count = n(),
    Percent = round(100 * Count / nrow(data), 2),
    Mean_Price = round(mean(price, na.rm = TRUE), 0),
    .groups = "drop"
  ) |>
  group_by(Variable) |>
  slice_max(order_by = Count, n = top_n_per_var, with_ties = FALSE) |>
  ungroup()

kable(
  nominal_summary,
  caption = paste0(
    "Top ", top_n_per_var,
    " categories per factor variable (counts, share %, and mean price)"
  ),
  digits = 2,
  align = c("l", "l", "r", "r", "r")
)
```

#### Interpretation

**city**: Most listings in Houston, Tucson, Phoenix, and Los Angeles. Prices range widely â€” highest in Los Angeles (\~\$1.9M), lowest around \$250k (Saint Louis).

**state**: California, Texas, and Florida dominate listings (\>30% total). California shows the highest mean price (\~\$1.1M).

**status**: 55% for sale, 45% sold. Active listings are priced higher (\~\$621k vs. \$515k).

**Overall**: Listings cluster in major U.S. cities and states, with strong regional price differences, especially high in California and large metro areas.

### Visualisation of nominal variables (top categories)

```{r}
#| label: "Visualization Nominal Statistics"
#| tbl-cap: "Top categories for factor variables with counts, proportions, and mean price"
#| code-fold: true

nominal_summary <- nominal_summary |>
  group_by(Variable) |>
  mutate(Category = forcats::fct_reorder(Category, Count),
         Category = factor(Category, levels = unique(Category))) |>
  ungroup()

# Plot: Facets untereinander, mit eigener x-Skala und y-Skala pro Variable
ggplot(nominal_summary, aes(x = Count, y = Category, fill = Variable)) +
  geom_col(show.legend = FALSE, alpha = 0.8, width = 0.7) +
  facet_wrap(~ Variable, ncol = 1, scales = "free", drop = TRUE) +
  scale_x_continuous(labels = label_comma()) +   #Tausendertrennung, keine 1e+05
  theme_minimal() +
  labs(
    title = "Top Categories per Factor Variable",
    x = "Count",
    y = "Category"
  ) +
  theme(
    panel.spacing.y = unit(1, "lines"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8),
    plot.margin = ggplot2::margin(5, 15, 5, 5)
  )
```

# Bivariate Analysis

### Pairs Plot (all numeric variables)

```{r}
#| label: "Pairs Plot"
#| code-fold: true

set.seed(123)

data_num <- data |>
  janitor::clean_names() |>
  select(where(is.numeric)) |>
  slice_sample(n = 3000) |>
  mutate(across(everything(), log1p)) 

p <- ggpairs(
  data_num,
  progress = FALSE,
  upper = list(continuous = wrap("cor", size = 4, alignPercent = 0.8, stars = TRUE)),
  lower = list(continuous = wrap("points", alpha = 0.3, size = 0.7)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.7))
)

p +
  theme_minimal(base_size = 11) +
  theme(
    strip.text = element_text(size = 8, face = "bold"),
    panel.grid = element_blank(),
    axis.text = element_text(size = 8),
    axis.title = element_text(size = 9),
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5)
  ) +
  labs(title = "Pairs Plot (log-transformiert, n=3000)")
```

#### Interpretation

Very high Correlation (0.8) between Price and Price per Squaremeter: The pricier a house the more you pay per squaremeter, which is logical since the squaremeters arent the only criterion for price.

Also a minor correlations for price are found in the number of beds (0.34), baths (0.58) and the house size (0.549).

The number of beds and baths correlates with the house size.

Interestingly the Price per Squaremeter does not correlate with the house size.

The lot size interestingly does not correlate with any of the other variables.

### Price vs. House Size by Status

```{r}
#| label: "scatter_price_size"
#| tbl-cap: "Price vs. house size by listing status"
#| code-fold: true

# Stichprobe ziehen fÃ¼r Performance
set.seed(123)
sample_data <- data %>% sample_n(50000)

plot_ly(
  sample_data,
  x = ~house_size,
  y = ~price,
  color = ~status,
  type = "scatter",
  mode = "markers",
  alpha = 0.6
) %>%
  plotly::layout(
    title = list(text = "Relationship Between House Size and Price by Status"),
    xaxis = list(title = "House Size (sqft)"),
    yaxis = list(title = "Price ($)", type = "log")
  )
```

#### Interpretation

**Positive relationship**: Larger houses generally have **higher prices**, though the relationship weakens for very large properties.

**Status comparison**: Both for_sale and sold homes follow **similar trends**, but **for_sale** listings appear higher in price, suggesting sellers may list above sale values.

**High variation**: At similar sizes, prices vary widely â€” showing the strong influence of **location** and other factors.

**Outliers**: A few extremely large or expensive properties stretch the scale upward.



### Average Property Price Map

```{r}
#| label: "state_mapping"
#| code-fold: true

valid_states <- tibble(
  state_name = c(state.name, "District of Columbia"),
  state_abbr = c(state.abb,  "DC")
)
```

```{r}
#| label: "map_avg_price"
#| tbl-cap: "Average property price by U.S. state"
#| code-fold: true

map_price <- data |>
  group_by(state) |>
  summarise(avg_price = mean(price, na.rm = TRUE), .groups = "drop") |>
  inner_join(valid_states, by = c("state" = "state_name")) |>
  mutate(avg_price_k = avg_price / 1000)

plot_ly(
  map_price,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_price_k,
  text = ~paste0(state, "<br>Avg Price: $", round(avg_price_k, 1), "K"),
  colorscale = list(c(0, 1), c("lightblue", "darkblue")),
  colorbar = list(title = "Avg Price ($K)")
) |>
  plotly::layout(
    title = list(text = "Average Property Price by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )

```

#### Interpretation

**Regional variation**: Western and coastal states show generally **higher property prices**, while central regions are lower.

**Highest averages**: States like **California, New York, and Washington** stand out with mean prices well above **\$1M**.

**Moderate prices**: States such as Texas, Florida, and Arizona fall in the **mid-range** (\~\$400â€“650K).

**Lower averages**: Midwest and Southern states have more affordable properties on average.

**Summary**: Property values are heavily influenced by **geography** â€” with the highest prices concentrated along the coasts and major urban centers.

### Average House Size Map

```{r}
#| label: "map_avg_size"
#| tbl-cap: "Average house size by U.S. state"
#| code-fold: true

map_size <- data %>%
  group_by(state) %>%
  summarise(avg_size = mean(house_size, na.rm = TRUE), .groups = "drop") %>%
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_size,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~avg_size,
  text = ~paste0(state, "<br>Avg Size: ", round(avg_size), " sqft"),
  colorscale = list(c(0, 1), c("lightgreen", "darkgreen")),
  colorbar = list(title = "Avg Size (sqft)")
) %>%
  plotly::layout(
    title = list(text = "Average House Size by U.S. State"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**General trend**: Average house sizes are fairly consistent across most states, typically around 2,000â€“2,500 sqft.

**Larger homes**: Some central and mountain states (e.g., Colorado, Utah, Iowa) show slightly larger averages, possibly due to more available land.

**Smaller homes**: Coastal and densely populated states (e.g., New York, California) tend to have smaller average house sizes.

### Price Range by US State

```{r}
#| label: "map_extreme_price"
#| tbl-cap: "Price range (max âˆ’ min) by U.S. state"
#| code-fold: true

map_extremes <- data |>
  group_by(state) |>
  summarise(
    min_price = suppressWarnings(min(price, na.rm = TRUE)),
    max_price = suppressWarnings(max(price, na.rm = TRUE)),
    .groups = "drop"
  ) |>
  mutate(range_price = max_price - min_price) |>
  inner_join(valid_states, by = c("state" = "state_name"))

plot_ly(
  map_extremes,
  type = "choropleth",
  locationmode = "USA-states",
  locations = ~state_abbr,
  z = ~range_price,
  text = ~paste0(
    state,
    "<br>Min: $", formatC(min_price, big.mark = ",", format = "f", digits = 0),
    "<br>Max: $", formatC(max_price, big.mark = ",", format = "f", digits = 0)
  ),
  colorscale = "Reds",
  colorbar = list(title = "Price Range ($)")
) |>
  plotly::layout(
    title = list(text = "Price Extrem Values by U.S. State (Max âˆ’ Min)"),
    geo = list(scope = "usa", projection = list(type = "albers usa"))
  )
```

#### Interpretation

**Highest ranges**: California shows by far the **largest price range** (over \$400M), driven by extremely high luxury property values.

**Moderate ranges**: States like Florida and parts of the Northeast also show wide **price spreads**, reflecting diverse markets from affordable to luxury homes.

**Lower ranges**: Most central and midwestern states have smaller price gaps, indicating more uniform housing markets.

# Data Engineering

Split the data into trainings data and test data

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```


### Linear Regression

```{r}
#| code-fold: true

model_lr <- lm(price ~ house_size + bath + bed + state + status, data = train_data)
tidy(model_lr) |>
  arrange(p.value) |>
  mutate(
    estimate = round(estimate, 1),
    std.error = round(std.error, 1),
    statistic = round(statistic, 1),
    p.value = signif(p.value, 3)
  ) |>
  datatable(
    caption = "Regressionsergebnisse (interaktiv)",
    filter = "top",         
    options = list(
      pageLength = 10,      
      autoWidth = TRUE,
      responsive = TRUE
    )
  )
```

```{r}
#| label: "Graphic Regression"
#| code-fold: true
 
tidy(model_lr, conf.int = TRUE) |>
  filter(term != "(Intercept)") |>
  mutate(term = reorder(term, estimate)) |>
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Greatest contributors to price",
    x = "regression coefficient",
    y = ""
  )
```

The cities are exempt from this regression as the number of distinct variables would be too large for the model. Therefore for location only states will be used.

Locations like US insular areas like Hawaii and the Virgin Islands appear to be the greatest contributors to price, California being third.

Regarding the numeric variables the number of baths increases housing price the most with around 384.000\$ per bath room. Every square meter of the house size adds 16.7\$ to the price, each acre of the lot around 14.2\$. Interestingly each additional bed appears to decrease the price by around 81.946\$. At this point it has to be noted, that these price changes assume that for each variable all other variables would be constant.

### Interpretation of Linear Regression

```{r}
# Vorhersagen auf Testdaten
lm_preds <- predict(model_lr, newdata = test_data)

# Kennwerte berechnen
lm_metrics <- tibble(
  RMSE = sqrt(mean((test_data$price - lm_preds)^2)),
  MAE  = mean(abs(test_data$price - lm_preds)),
  R2   = 1 - sum((test_data$price - lm_preds)^2) /
              sum((test_data$price - mean(test_data$price))^2)
)

lm_metrics
```
The linear regression model shows limited predictive performance.

An RMSE of 1,018,695 indicates large average prediction errors, reflecting the modelâ€™s inability to capture complex patterns in housing prices.
The MAE of 329,149 suggests that, on average, predictions deviate by roughly 329k from the true prices.

An RÂ² of 0.204 means that the model explains only 20.4% of the variance in prices, indicating that the assumed linear relationships between the predictors and the target variable are insufficient to describe the underlying structure of the data.

Overall, the linear regression serves as a baseline model with high interpretability but substantially lower explanatory and predictive power compared to the Random Forest model.

### Hyperparameter for Random Forest

```{r}
"""
set.seed(123)

# einfache Grid-Suche
rf_grid <- expand.grid(
  num.trees = c(100, 300, 500),
  mtry      = c(2, 3, 4)
)

rf_results <- rf_grid |>
  mutate(
    rmse = map2_dbl(num.trees, mtry, ~ {
      model <- ranger(
        price ~ house_size + bed + bath + acre_lot + state + status,
        data = train_data,
        num.trees = .x,
        mtry = .y,
        importance = "impurity"
      )
      preds <- predict(model, data = test_data)$predictions
      sqrt(mean((test_data$price - preds)^2))
    })
  ) |>
  arrange(rmse)

rf_results
"""
```

### Interpretation for Hyperparameters of Random Forest

Before interpreting the results, it is useful to briefly clarify the meaning of the two Random Forest hyperparameters:

**num.trees**
Specifies the number of decision trees grown in the forest.
Increasing num.trees generally reduces variance and stabilizes predictions, but beyond a certain point the performance gains become marginal while computational cost increases.

**mtry**
Defines the number of predictor variables randomly selected as candidates at each split.
Smaller values increase randomness and tree diversity, which can reduce overfitting, whereas larger values make individual trees more similar and may increase variance.

  num.trees mtry     rmse
1       300    2 838263.1
2       500    2 839089.1
3       100    2 846528.8
4       500    3 860607.9
5       100    3 863708.7
6       300    3 867879.5
7       100    4 889648.8
8       300    4 896180.3
9       500    4 902455.2

The grid search results show clear performance differences across combinations of num.trees and mtry:

The best-performing configuration is
**num.trees = 300** and **mtry = 2**, achieving the lowest **RMSE (838,263.1).**

Increasing the number of trees from 300 to 500 with the same mtry = 2 does not improve performance (RMSE slightly increases to 839,089.1), indicating diminishing returns from adding more trees.

Using only 100 trees leads to a noticeably higher RMSE (846,528.8), suggesting that the forest is not yet sufficiently stable.

Higher values of mtry (3 or 4) consistently result in worse RMSE, regardless of the number of trees. This indicates that allowing too many variables at each split reduces tree diversity and increases overfitting.

## Random Forest

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)
rf_model <- ranger(
  formula = price ~ house_size + bed + bath + acre_lot + state + status,
  data = train_data,
  importance = "impurity",
  num.trees = 300,
  mtry = 2
)

importance_df <- enframe(rf_model$variable.importance, name = "Variable", value = "Importance") %>%
  arrange(desc(Importance))

ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance, fill = Importance)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Variable Importance from Random Forest Model",
    x = "Variable",
    y = "Importance"
  )

```

### Interpretation of Random Forest Model

```{r}
# Vorhersagen auf Testdaten
rf_preds <- predict(rf_model, data = test_data)$predictions

# Kennwerte berechnen
rf_metrics <- tibble(
  RMSE = sqrt(mean((test_data$price - rf_preds)^2)),
  MAE  = mean(abs(test_data$price - rf_preds)),
  R2   = 1 - sum((test_data$price - rf_preds)^2) /
              sum((test_data$price - mean(test_data$price))^2)
)

rf_metrics
```

The Random Forest model achieves an **RMSE of 842,861**, indicating a relatively large average prediction error, which is expected given the high variance and scale of housing prices.

The **MAE of 224,475** shows that, on average, predictions deviate from the true prices by approximately **224k**, making it a more robust indicator of typical error magnitude than RMSE.

An **RÂ² of 0.455** means that the model explains about 45.5% of the variance in housing prices, indicating moderate explanatory power and suggesting that while the model captures important patterns, a substantial portion of price variability remains unexplained.

### Hyperparameter for Neural Network

```{r}
"""
nn_grid <- expand.grid(
  size  = c(3, 5, 7),   # Anzahl Hidden Neurons
  decay = c(0, 0.01, 0.1)
)

nn_results <- nn_grid |>
  mutate(
    rmse = map2_dbl(size, decay, ~ {
      model <- nnet(
        price ~ house_size + bed + bath + acre_lot + state + status,
        data = train_data,
        size = .x,
        decay = .y,
        linout = TRUE,
        maxit = 500,
        trace = FALSE
      )
      preds <- predict(model, test_data)
      sqrt(mean((test_data$price - preds)^2))
    })
  ) |>
  arrange(rmse)

nn_results
"""
```

### Interpretation for Hyperparameters

Before interpreting the results, it is useful to briefly clarify the meaning of the two neural network hyperparameters:

**size**
Specifies the number of neurons in the hidden layer.
A larger size increases the modelâ€™s capacity to learn complex, non-linear relationships, but also raises the risk of overfitting and increases computational complexity.

**decay**
Controls the strength of weight decay (L2 regularization).
Higher values of decay penalize large weights more strongly, which can improve generalization by reducing overfitting, while a value of zero corresponds to no regularization.

  size decay    rmse
1    7  0.10 1100793
2    7  0.00 1141877
3    7  0.01 1141878
4    3  0.00 1141878
5    5  0.01 1141878
6    3  0.10 1141878
7    3  0.01 1141878
8    5  0.00 1141878
9    5  0.10 1141878

The grid search results indicate limited sensitivity of model performance to most combinations of size and decay, with one notable exception.
The best-performing configuration is **size = 7** and **decay = 0.10**, achieving the **lowest RMSE (1,100,793)**.
This suggests that a larger hidden layer combined with relatively strong regularization allows the neural network to capture more complex relationships in the data while mitigating overfitting.

## Neural Network

```{r}
#| code-fold: true
#| warning: false
#| message: false

set.seed(123)

nn_model <- nnet(
  price ~ house_size + bed + bath + acre_lot + state + status,
  data = train_data,
  size = 7,
  decay = 0.10,
  linout = TRUE,
  maxit = 500,
  trace = FALSE
)
```


## Comparision of the Models



# Conclusion

